{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e091e3e-4131-4661-8f8b-e8f27706fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: rich in c:\\users\\admin\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\admin\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\admin\\anaconda3\\lib\\site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openvino'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Intel specific imports\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenvino\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IECore\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdaal4py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_regression_training, linear_regression_prediction\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmkl\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openvino'"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib numpy scikit-learn tensorflow keras\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import IPython\n",
    "import IPython.display\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import keras.layers as kl\n",
    "import keras.activations as ka\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Intel specific imports\n",
    "from openvino.inference_engine import IECore\n",
    "from daal4py import linear_regression_training, linear_regression_prediction\n",
    "import mkl\n",
    "import oneapi\n",
    "import vtune\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d221419-515f-466f-9ec8-d7d6966579c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "SKIP_TIMESTEPS = 20\n",
    "FORECAST_WINDOW = 20\n",
    "FORECAST_SHIFT = 10\n",
    "CONV_WIDTH = 5\n",
    "TARGET_LABELS = [\"ph\", \"temperature\", \"disolved_oxg\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f15b7af-1e93-4871-9dfa-af5c4938c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340fd3a8-c225-4da7-98b0-746d352bb6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IoTpond1.csv', 'IoTPond2.csv', 'IoTPond3.csv', 'IoTPond4.csv']\n"
     ]
    }
   ],
   "source": [
    "base_path = \"D:/Aquaponics dataset\"\n",
    "dataset_folder = \"Cleaned pond dataset\"\n",
    "f_path = os.path.join(base_path, dataset_folder)\n",
    "ponds = os.listdir(f_path)[:]\n",
    "print(ponds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eabac55-98af-4c5a-9b26-d7c4fa6fd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import IPython\n",
    "\n",
    "def load_correct_data(ponds, skip_timesteps):\n",
    "    data = []\n",
    "    date_times = []\n",
    "    used_ponds = []\n",
    "    unused_ponds = []\n",
    "\n",
    "    # Loading data\n",
    "    for pond in ponds:\n",
    "        try:\n",
    "            df = pd.read_csv(f_path + pond)\n",
    "            df = df[::skip_timesteps]\n",
    "            df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "            date_time1 = df.pop(\"created_at\")\n",
    "            df.drop(columns=['population', 'entry_id'], inplace=True)\n",
    "            df.fillna(df.mean(), inplace=True)\n",
    "            df.drop(columns=[col for col in df.columns if 'Unnamed' in col], inplace=True)\n",
    "\n",
    "            data.append(df)\n",
    "            date_times.append(date_time1)\n",
    "            used_ponds.append(pond)\n",
    "            IPython.display.clear_output()\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR at POND: {pond} - {e}\")\n",
    "            unused_ponds.append(pond)\n",
    "\n",
    "    # Correcting data\n",
    "    for df in data:\n",
    "        df['temperature'] = df['temperature'].clip(lower=20)\n",
    "        df['ph'] = df['ph'].clip(lower=5, upper=12)\n",
    "        df['ammonia'] = df['ammonia'].clip(upper=10)\n",
    "        df['nitrate'] = df['nitrate'].clip(upper=2000)\n",
    "        IPython.display.clear_output()\n",
    "\n",
    "    return data, date_times, used_ponds, unused_ponds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffc4f445-aba3-43f3-98a8-0f7cef5d8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def standardize_normalize(df, method='standardize'):\n",
    "    df_cols = df.columns\n",
    "    \n",
    "    if method == 'standardize':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'normalize':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'standardize' or 'normalize'\")\n",
    "    \n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=df_cols)\n",
    "    \n",
    "    return df_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dd98197-1ccd-42a6-8a25-1aab3f208720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df):\n",
    "    \"\"\"\n",
    "    Compute mean, standard deviation, maximum, and minimum for each column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing four DataFrames (mean, std, max, min).\n",
    "    \"\"\"\n",
    "    df_mean = df.mean()\n",
    "    df_std = df.std()\n",
    "    df_max = df.max()\n",
    "    df_min = df.min()\n",
    "    \n",
    "    return df_mean, df_std, df_max, df_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6bfbdb6-d2ff-4dc9-b96b-d1d898a6a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def destandardize_denormalize(df, transformations):\n",
    "    \"\"\"\n",
    "    Apply inverse transformations to de-standardize and de-normalize the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The standardized and normalized DataFrame.\n",
    "    transformations (tuple): A tuple containing the normalizer and standardizer.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The original DataFrame after inverse transformations.\n",
    "    \"\"\"\n",
    "    normalizer, standardizer = transformations\n",
    "    df_cols = df.columns\n",
    "    \n",
    "    # Apply inverse transformations in the correct order\n",
    "    df = standardizer.inverse_transform(df)\n",
    "    df = normalizer.inverse_transform(df)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    df = pd.DataFrame(df, columns=df_cols)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e46c19bc-3fb9-454c-be0c-be2c49062c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_feature(feature_idx, data, date_times, used_ponds):\n",
    "    \"\"\"\n",
    "    Visualize a specific feature across multiple ponds with percentile thresholds.\n",
    "\n",
    "    Parameters:\n",
    "    feature_idx (int): Index of the feature to visualize.\n",
    "    data (list of pd.DataFrame): List of DataFrames containing pond data.\n",
    "    date_times (list of pd.Series): List of date-time Series corresponding to each pond.\n",
    "    used_ponds (list of str): List of pond names.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    num_ponds = len(data)\n",
    "    nrows = (num_ponds + 1) // 2\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=2, figsize=(16, 5 * nrows))\n",
    "\n",
    "    for i, df in enumerate(data):\n",
    "        row_idx = i // 2\n",
    "        col_idx = i % 2\n",
    "\n",
    "        test_feature = df.columns[feature_idx]\n",
    "        thresholds = [np.percentile(df[test_feature], p) for p in [20, 50, 75, 90]]\n",
    "        colors = [\"black\", \"green\", \"yellow\", \"red\"]\n",
    "\n",
    "        axs[row_idx, col_idx].plot(date_times[i], df[test_feature])\n",
    "        for threshold, color in zip(thresholds, colors):\n",
    "            axs[row_idx, col_idx].axhline(threshold, color=color)\n",
    "        axs[row_idx, col_idx].set_title(f\"{used_ponds[i]} [{thresholds[0]:.2f} - {thresholds[3]:.2f}]\")\n",
    "\n",
    "    fig.suptitle(test_feature)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da1fe745-ec01-4c6d-a3aa-f32410cf2f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_df(df_idx, data, date_times):\n",
    "    df = data[df_idx]\n",
    "    date_time = date_times[df_idx]\n",
    "    plot_cols = df.columns\n",
    "\n",
    "    colors = ['blue', 'red', 'green', 'yellow', 'purple', 'orange', 'cyan', 'magenta']\n",
    "\n",
    "    plot_features = df[plot_cols]\n",
    "    plot_features.index = date_time\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(15, 10))\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    for i, col in enumerate(plot_features):\n",
    "        row_idx = i // 2\n",
    "        col_idx = i % 2\n",
    "        axs[row_idx, col_idx].plot(plot_features[col], color=colors[i], label=col)\n",
    "        axs[row_idx, col_idx].tick_params(axis='x', labelsize=7)\n",
    "        axs[row_idx, col_idx].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1e9fb57-d22f-4d29-ae54-47a4034890cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plot the training and evaluation loss history.\n",
    "\n",
    "    Parameters:\n",
    "    history (dict): Dictionary containing 'loss' and 'val_loss' keys with their respective values.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if 'loss' not in history or 'val_loss' not in history:\n",
    "        raise ValueError(\"The history dictionary must contain 'loss' and 'val_loss' keys.\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Evaluation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Evaluation Loss Over Epochs\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eec491a2-2543-4a3c-a6cf-735ce4892132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_ratio=0.8, val_ratio=0.9):\n",
    "    \"\"\"\n",
    "    Split data into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    data (list of pd.DataFrame): List of DataFrames to be split.\n",
    "    train_ratio (float): Proportion of data to be used for training.\n",
    "    val_ratio (float): Proportion of data to be used for validation.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Three lists containing the training, validation, and test DataFrames.\n",
    "    \"\"\"\n",
    "    if not (0 < train_ratio < 1) or not (0 < val_ratio < 1):\n",
    "        raise ValueError(\"train_ratio and val_ratio must be between 0 and 1.\")\n",
    "    if train_ratio >= val_ratio:\n",
    "        raise ValueError(\"train_ratio must be less than val_ratio.\")\n",
    "\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for df in data:\n",
    "        n = len(df)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = int(n * val_ratio)\n",
    "        \n",
    "        train_data.append(df[:train_end])\n",
    "        val_data.append(df[train_end:val_end])\n",
    "        test_data.append(df[val_end:])\n",
    "    \n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd7b5c6f-be99-4de7-a812-4ac3b4f52e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from TensorFlow\n",
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift,\n",
    "                 train_data, val_data, test_data,\n",
    "                 label_columns=None):\n",
    "        # Store the raw data.\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_data[0].columns)}\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "\n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes manually.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def plot(self, model=None, plot_col=None, max_subplots=3):\n",
    "        if plot_col is None:\n",
    "            raise ValueError(\"plot_col must be specified.\")\n",
    "        inputs, labels = self.example\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plot_col_index = self.column_indices[plot_col]\n",
    "        max_n = min(max_subplots, len(inputs))\n",
    "        for n in range(max_n):\n",
    "            plt.subplot(max_n, 1, n+1)\n",
    "            plt.ylabel(f'{plot_col} [normed]')\n",
    "            plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                     label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "            if self.label_columns:\n",
    "                label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "            else:\n",
    "                label_col_index = plot_col_index\n",
    "\n",
    "            if label_col_index is None:\n",
    "                continue\n",
    "\n",
    "            if model is not None:\n",
    "                predictions = model(inputs)\n",
    "                plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                            marker='X', edgecolors='k', label='Predictions',\n",
    "                            c='#ff7f0e', s=64)\n",
    "            plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                        edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "\n",
    "            if n == 0:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.xlabel(plot_col)\n",
    "        plt.show()\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        full_dataset = self.make_dataset(self.train_data[0])\n",
    "        for i in range(1, len(self.train_data)):\n",
    "            full_dataset = full_dataset.concatenate(self.make_dataset(self.train_data[i]))\n",
    "\n",
    "        return full_dataset\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Create and return the test dataset.\n",
    "        \"\"\"\n",
    "        full_dataset = self.make_dataset(self.test_data[0])\n",
    "        for i in range(1, len(self.test_data)):\n",
    "            full_dataset = full_dataset.concatenate(self.make_dataset(self.test_data[i]))\n",
    "\n",
    "        return full_dataset\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "        \"\"\"\n",
    "        Get and cache an example batch of `inputs, labels` for plotting.\n",
    "        \"\"\"\n",
    "        result = getattr(self, '_example', None)\n",
    "        if result is None:\n",
    "            # No example batch was found, so get one from the `.train` dataset\n",
    "            result = next(iter(self.train))\n",
    "      # And cache it for next time\n",
    "            self._example = result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "878a32b8-0dd0-4feb-9a2f-2f2f4b44b715",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_correct_data() missing 2 required positional arguments: 'ponds' and 'skip_timesteps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data, date_times, used_ponds, unused_ponds \u001b[38;5;241m=\u001b[39m load_correct_data()\n",
      "\u001b[1;31mTypeError\u001b[0m: load_correct_data() missing 2 required positional arguments: 'ponds' and 'skip_timesteps'"
     ]
    }
   ],
   "source": [
    "data, date_times, used_ponds, unused_ponds = load_correct_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0236b5d9-8883-47cf-b0a9-1a26a8849676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(used_ponds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6e51a-1481-43cc-8c8f-5265b63da12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9acad2-9509-4ae7-aa59-971d01453992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing and standarizing the data based on whole dataset\n",
    "full_data = pd.concat(data)\n",
    "full_data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044ff75-b281-4598-97e1-adc30e78b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data, normalizer, standarizer = standarize_normalize(full_data)\n",
    "full_data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83aa23-c479-4d28-87f7-6817743410e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    df_cols = data[i].columns\n",
    "    data[i] = normalizer.transform(data[i])\n",
    "    data[i] = standarizer.transform(data[i])\n",
    "\n",
    "    data[i] = pd.DataFrame(data[i], columns=df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f3ee1-10bb-4c64-aca4-ec7e34bd8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ec7de-9b70-4b2a-8275-8c05e47f81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing a dataframe\n",
    "visualize_df(3, data, date_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e18e1-86ef-4c99-aeae-9482644e0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset into train, val, test sets\n",
    "train_data, val_data, test_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e7c08-bbb4-441e-9650-4f0c1f107d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(data[0].columns)}\n",
    "column_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e3a14-3473-44c1-9aae-a87de0317b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del date_times\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
